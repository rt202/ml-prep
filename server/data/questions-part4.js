// Part 4: Big Data, MLOps, System Design, A/B Testing, Business, Behavioral, Cloud, RecSys, Time Series, Ethics
export const questionsPart4 = [
  // ====== UNIT: big-data ======
  { id: 'bd-1', text: 'What is Apache Spark and why is it used for big data processing?', options: ['A database', 'A distributed computing framework that processes large datasets in parallel across a cluster, with in-memory computation that is much faster than MapReduce', 'A message queue', 'A visualization tool'], correctAnswer: 1, explanation: 'Spark processes data across a cluster using RDDs (Resilient Distributed Datasets) and DataFrames. It is 10-100× faster than Hadoop MapReduce due to in-memory computation. Supports batch, streaming, ML (MLlib), graph processing, and SQL.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-2', text: 'What is MapReduce?', options: ['A sorting algorithm', 'A programming model for distributed processing: Map phase transforms data in parallel, Reduce phase aggregates results', 'A database query language', 'A machine learning algorithm'], correctAnswer: 1, explanation: 'MapReduce processes large datasets in two phases: Map (applies a function to each element in parallel, producing key-value pairs) and Reduce (aggregates values by key). Hadoop implements MapReduce. Spark improved upon it with in-memory processing.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-3', text: 'What is the CAP theorem?', options: ['A theorem about data caps', 'In a distributed system, you can only guarantee two of three: Consistency, Availability, and Partition tolerance', 'A theorem about capping resources', 'A machine learning theorem'], correctAnswer: 1, explanation: 'CAP theorem states distributed systems can only guarantee 2 of 3: Consistency (all nodes see same data), Availability (every request gets a response), Partition tolerance (system works despite network failures). Most systems choose AP or CP.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-4', text: 'What is data partitioning/sharding?', options: ['Deleting data', 'Splitting data across multiple nodes/files based on a key to enable parallel processing and improve query performance', 'Compressing data', 'Backing up data'], correctAnswer: 1, explanation: 'Partitioning divides data across nodes: hash partitioning (consistent distribution), range partitioning (by value ranges), list partitioning (by category). Spark uses partitions for parallelism. Good partitioning minimizes data shuffling across the network.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-5', text: 'What is the difference between batch processing and stream processing?', options: ['They are the same', 'Batch processes large volumes of accumulated data at intervals; stream processing handles data in real-time as it arrives, with lower latency but more complexity', 'Batch is always faster', 'Stream processing cannot handle large volumes'], correctAnswer: 1, explanation: 'Batch: process accumulated data periodically (Spark, Hadoop). Stream: process data continuously in real-time (Kafka Streams, Flink, Spark Streaming). Lambda architecture combines both. Stream is needed for real-time ML (fraud detection, recommendations).', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-6', text: 'What is Apache Kafka used for?', options: ['Data visualization', 'A distributed event streaming platform for building real-time data pipelines and streaming applications, acting as a durable message broker', 'A machine learning library', 'A database'], correctAnswer: 1, explanation: 'Kafka is a distributed, fault-tolerant streaming platform. It handles real-time data feeds as topics of messages. Used for: log aggregation, event sourcing, real-time analytics, ML feature pipelines. Supports pub-sub and queue patterns.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-7', text: 'What is data lakehouse architecture?', options: ['A data storage on a lake', 'An architecture combining data lake flexibility (raw data, any format) with data warehouse reliability (ACID transactions, schema enforcement), implemented by Delta Lake, Iceberg', 'A new database type', 'A data visualization approach'], correctAnswer: 1, explanation: 'Data lakehouses combine data lake storage (cheap, flexible, any format) with warehouse features (ACID transactions, schema evolution, time travel). Technologies: Delta Lake (Databricks), Apache Iceberg, Apache Hudi. Replaces the need for separate lake and warehouse.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-8', text: 'What is data skew and how does it affect distributed processing?', options: ['Skewed distributions', 'When data is unevenly distributed across partitions, causing some tasks to take much longer than others, creating bottlenecks', 'Data corruption', 'A feature of Spark'], correctAnswer: 1, explanation: 'Data skew means some partitions have much more data than others (e.g., popular keys). This creates stragglers (slow tasks) that bottleneck the entire job. Solutions: salting (adding random prefix to keys), broadcast joins for small tables, custom partitioning.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-9', text: 'What is the difference between ETL and ELT?', options: ['They are the same', 'ETL transforms data before loading (traditional); ELT loads raw data first then transforms in the destination (modern cloud approach, more flexible)', 'ELT is always slower', 'ETL is only for streaming'], correctAnswer: 1, explanation: 'ETL: Extract, Transform, Load (transform in staging area before loading). ELT: Extract, Load, Transform (load raw data, transform in-place using destination compute). ELT is preferred with modern cloud warehouses (BigQuery, Snowflake) that have powerful compute.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'distributed-computing' },
  { id: 'bd-10', text: 'What is a DAG in the context of data pipelines?', options: ['A debugging tool', 'A Directed Acyclic Graph representing the workflow of tasks with dependencies, ensuring tasks execute in the correct order without circular dependencies', 'A data format', 'A type of database'], correctAnswer: 1, explanation: 'DAG (Directed Acyclic Graph) defines task dependencies in data pipelines. Tools like Airflow, Dagster, and Prefect use DAGs to orchestrate complex workflows. Each node is a task, edges define execution order. "Acyclic" means no circular dependencies.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },

  // spark-pipelines lesson (remaining questions for big-data unit)
  { id: 'sp-1', text: 'What is the difference between Spark DataFrame and RDD?', options: ['They are identical', 'DataFrames have schema/structure with SQL-like API and Catalyst optimizer; RDDs are lower-level unstructured distributed collections. DataFrames are faster due to optimization.', 'RDDs are newer and better', 'DataFrames cannot handle complex types'], correctAnswer: 1, explanation: 'RDDs (Resilient Distributed Datasets) are the original Spark abstraction—type-safe but without optimization. DataFrames add schema, SQL API, and the Catalyst optimizer for query planning. DataFrames are 2-10× faster. Use DataFrames unless you need fine-grained control.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-2', text: 'What are Spark transformations vs. actions?', options: ['They are the same', 'Transformations are lazy operations that define a computation plan (map, filter, join); actions trigger execution and return results (collect, count, save)', 'Actions are always faster', 'Transformations execute immediately'], correctAnswer: 1, explanation: 'Transformations (map, filter, groupBy, join) are lazy—they build a logical plan. Actions (count, collect, write, show) trigger actual execution. Spark optimizes the entire transformation chain before executing. This lazy evaluation enables efficient query planning.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-3', text: 'What is data versioning and why is it important for ML?', options: ['Git for code only', 'Tracking versions of datasets over time to ensure reproducibility, enabling rollback, auditing data changes, and reproducing model training', 'Database version numbers', 'Only for production data'], correctAnswer: 1, explanation: 'Data versioning (DVC, Delta Lake time travel, LakeFS) tracks dataset changes like Git tracks code. Essential for: reproducing experiments, debugging model issues, regulatory compliance, rolling back bad data updates. Without it, ML experiments are not reproducible.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-4', text: 'What is Apache Airflow used for?', options: ['Air quality monitoring', 'Orchestrating complex data pipelines by defining workflows as DAGs with scheduling, monitoring, retry logic, and dependency management', 'A streaming framework', 'A machine learning library'], correctAnswer: 1, explanation: 'Airflow is a workflow orchestration platform. You define DAGs in Python, specifying tasks, dependencies, schedules, and retry policies. It provides a UI for monitoring, logging, and triggering workflows. Common in MLOps for training pipelines, ETL jobs.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-5', text: 'What is the difference between columnar and row-based storage?', options: ['They store the same way', 'Row-based stores rows together (good for transactional writes); columnar stores columns together (good for analytical queries that scan specific columns)', 'Columnar is always faster', 'Row-based is only for relational databases'], correctAnswer: 1, explanation: 'Row-based (PostgreSQL, MySQL): stores entire rows together. Good for OLTP (inserts, updates, point queries). Columnar (Parquet, ORC, BigQuery): stores columns together. Good for OLAP (aggregations, scans on few columns). Columnar enables better compression too.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-6', text: 'What is a shuffle in Spark and why is it expensive?', options: ['Randomizing data', 'Redistributing data across partitions for operations like groupBy/join, requiring data transfer across the network, disk I/O, and serialization', 'Sorting data', 'A type of caching'], correctAnswer: 1, explanation: 'Shuffles occur when data must be rearranged across partitions (groupBy, join, distinct, repartition). They involve: serialization, writing to disk, network transfer, deserialization. Shuffles are the most expensive Spark operations. Minimize by: broadcast joins, pre-partitioning.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-7', text: 'What file format is most commonly used for big data ML pipelines and why?', options: ['CSV', 'Parquet: a columnar format with efficient compression, predicate pushdown, schema evolution, and fast reads for analytical queries', 'JSON', 'XML'], correctAnswer: 1, explanation: 'Parquet is the standard for big data/ML: columnar (fast column selection), compressed (snappy/gzip), supports schema evolution, predicate pushdown (skip irrelevant data), and is natively supported by Spark, Pandas, and most data tools. Much more efficient than CSV/JSON.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },
  { id: 'sp-8', text: 'What is feature pipeline orchestration?', options: ['Manual feature creation', 'Automating the end-to-end process of extracting, transforming, and serving features for ML models, ensuring consistency between training and inference', 'A feature selection method', 'GPU orchestration'], correctAnswer: 1, explanation: 'Feature pipeline orchestration automates: data ingestion → transformation → feature computation → storage (feature store) → serving. Tools: Airflow, Dagster, Prefect. Critical for ensuring training-serving consistency and feature freshness in production ML systems.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'big-data', lessonId: 'spark-pipelines' },

  // ====== UNIT: mlops ======
  { id: 'mlo-1', text: 'What is MLOps?', options: ['Machine Learning Operations only', 'A set of practices that combines ML, DevOps, and data engineering to reliably deploy and maintain ML systems in production, covering the full ML lifecycle', 'A specific tool', 'Only model deployment'], correctAnswer: 1, explanation: 'MLOps encompasses: experiment tracking, data/model versioning, CI/CD for ML, automated training pipelines, model serving/monitoring, feature stores, and governance. It bridges the gap between ML development and reliable production deployment.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'mlops-fundamentals' },
  { id: 'mlo-2', text: 'What is experiment tracking and why is it important?', options: ['Tracking scientific experiments', 'Systematically logging hyperparameters, metrics, code versions, and artifacts for each ML experiment to enable reproducibility and comparison', 'Tracking user experiments', 'A/B test tracking'], correctAnswer: 1, explanation: 'Experiment tracking (MLflow, Weights & Biases, Neptune) logs: hyperparameters, metrics, code version, data version, model artifacts. Essential for: reproducibility, comparing experiments, auditing, collaboration. Without it, ML development becomes chaotic.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'mlops-fundamentals' },
  { id: 'mlo-3', text: 'What is a model registry?', options: ['A government registration for models', 'A centralized repository for managing model versions, stages (development/staging/production), metadata, and approval workflows', 'A list of all ML algorithms', 'A feature store'], correctAnswer: 1, explanation: 'Model registries (MLflow Model Registry, Vertex AI, SageMaker) store: model artifacts, version history, stage transitions (dev→staging→prod), metadata, lineage. They provide governance, approval workflows, and rollback capabilities.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'mlops-fundamentals' },
  { id: 'mlo-4', text: 'What is CI/CD for machine learning?', options: ['The same as software CI/CD', 'Continuous Integration (testing code, data, and model quality) and Continuous Delivery/Deployment (automated model training, validation, and deployment pipelines)', 'Only code testing', 'A deployment tool'], correctAnswer: 1, explanation: 'ML CI/CD extends software CI/CD with: data validation tests, model quality tests, training pipeline automation, model validation gates, automated deployment. CI: trigger on code/data changes, run tests. CD: auto-deploy passing models. Tools: GitHub Actions, Jenkins, Kubeflow.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'mlops-fundamentals' },
  { id: 'mlo-5', text: 'What is Docker and why is it important for ML deployment?', options: ['A documentation tool', 'A containerization platform that packages applications with all dependencies into portable containers, ensuring consistent environments across development, testing, and production', 'A virtual machine', 'A cloud provider'], correctAnswer: 1, explanation: 'Docker containers package code + dependencies + runtime into isolated, reproducible environments. For ML: ensures same Python/library versions everywhere, eliminates "works on my machine" issues, enables easy scaling. Dockerfiles define the container build process.', difficulty: 'easy', roles: ['ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'mlops-fundamentals' },
  { id: 'mlo-6', text: 'What is model serving and what are common patterns?', options: ['Serving food to models', 'Making trained models available for predictions; patterns include REST API serving, batch prediction, streaming prediction, and edge deployment', 'Only REST APIs', 'A training technique'], correctAnswer: 1, explanation: 'Model serving patterns: REST API (Flask, FastAPI, TFServing), gRPC (low latency), batch prediction (scheduled offline scoring), streaming (real-time with Kafka), edge (TFLite, ONNX on devices). Choice depends on latency requirements, throughput, and infrastructure.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'deployment-serving' },
  { id: 'mlo-7', text: 'What is canary deployment for ML models?', options: ['Testing in a coal mine', 'Gradually routing a small percentage of traffic to a new model version while monitoring metrics, before full rollout', 'Deploying to a staging environment', 'A/B testing'], correctAnswer: 1, explanation: 'Canary deployment routes a small % of traffic (e.g., 5%) to the new model while 95% goes to the current model. Monitor key metrics (latency, errors, business KPIs). If good, gradually increase traffic. If bad, roll back. Minimizes risk of bad model deployments.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'mlops', lessonId: 'deployment-serving' },
  { id: 'mlo-8', text: 'What is ONNX and why is it useful?', options: ['A programming language', 'An open format for representing ML models that enables interoperability between different frameworks (PyTorch, TensorFlow, etc.) and optimized inference', 'A database format', 'A training framework'], correctAnswer: 1, explanation: 'ONNX (Open Neural Network Exchange) provides a common format for ML models. Benefits: train in PyTorch, deploy with TensorRT or ONNX Runtime for optimized inference. ONNX Runtime provides cross-platform, hardware-optimized inference with often 2-5× speedup.', difficulty: 'hard', roles: ['ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'mlops', lessonId: 'deployment-serving' },
  { id: 'mlo-9', text: 'What is model drift and how do you detect it?', options: ['Models physically moving', 'Degradation of model performance over time due to changes in data distribution; detected by monitoring prediction distributions, feature distributions, and performance metrics', 'A training issue', 'A deployment error'], correctAnswer: 1, explanation: 'Types: data drift (input distribution changes), concept drift (relationship between input and output changes), prediction drift (output distribution changes). Detection: statistical tests (KS test, PSI), monitoring dashboards, performance metric alerts.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'monitoring-maintenance' },
  { id: 'mlo-10', text: 'What is Kubernetes and how is it used for ML workloads?', options: ['A programming language', 'A container orchestration platform that automates deployment, scaling, and management of containerized applications; used for ML to manage training jobs and serving infrastructure', 'A machine learning framework', 'A data storage system'], correctAnswer: 1, explanation: 'Kubernetes (K8s) orchestrates containers at scale. For ML: manages model serving pods (auto-scaling based on traffic), training job scheduling (GPU allocation), AB testing (traffic splitting), health checks, and rolling deployments. KubeFlow builds on K8s specifically for ML.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'mlops', lessonId: 'monitoring-maintenance' },

  // Remaining MLOps questions
  { id: 'mlo-11', text: 'What is a shadow deployment?', options: ['Deploying at night', 'Running a new model in parallel with the production model, sending the same traffic to both but only using the old model responses, to evaluate the new model safely', 'A backup deployment', 'Testing in staging'], correctAnswer: 1, explanation: 'Shadow/dark deployment: new model receives production traffic but its responses are logged (not served to users). Compare new model predictions against current model. This is the safest way to evaluate a new model with real traffic. No user impact if new model is bad.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'mlops', lessonId: 'deployment-serving' },
  { id: 'mlo-12', text: 'What are the key metrics to monitor for a production ML model?', options: ['Only accuracy', 'Model performance metrics, prediction distribution, feature distribution, latency, throughput, error rates, resource utilization, and business KPIs', 'Only server uptime', 'Only data volume'], correctAnswer: 1, explanation: 'Monitor: (1) Model metrics (accuracy, F1 on labeled data), (2) Prediction distribution shifts, (3) Feature distribution (data drift), (4) Operational (latency p50/p95/p99, throughput, errors), (5) Resource (CPU, GPU, memory), (6) Business metrics (conversion, revenue impact).', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'mlops', lessonId: 'monitoring-maintenance' },
  { id: 'mlo-13', text: 'What is the difference between online learning and batch retraining?', options: ['Online learning uses the internet', 'Online learning updates the model incrementally with each new data point; batch retraining retrains the entire model periodically on accumulated data', 'Batch retraining is always better', 'They produce identical models'], correctAnswer: 1, explanation: 'Batch retraining: retrain from scratch on new + old data periodically (daily/weekly). Simple, reliable. Online learning: update model continuously with streaming data. Faster adaptation but harder to implement, debug, and can be unstable. Choose based on how fast the data distribution changes.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'mlops', lessonId: 'monitoring-maintenance' },

  // ====== UNIT: system-design ======
  { id: 'sd-1', text: 'How would you design a recommendation system for an e-commerce platform?', options: ['Use a single algorithm', 'Multi-stage system: candidate generation (collaborative filtering, content-based), ranking (ML model with user/item features), re-ranking (business rules, diversity), with online/offline components', 'Just use popular items', 'Random recommendations'], correctAnswer: 1, explanation: 'Production RecSys: (1) Candidate generation: retrieve ~1000 candidates from millions (ANN, collaborative filtering). (2) Ranking: score candidates with a feature-rich ML model. (3) Re-ranking: apply business rules, diversity, freshness. (4) Feature store for real-time features.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'case_study', companySizes: ['large', 'faang'], unitId: 'system-design', lessonId: 'design-principles' },
  { id: 'sd-2', text: 'What are the key components of an ML system design?', options: ['Just the model', 'Data collection/storage, feature engineering pipeline, model training, evaluation, deployment, monitoring, and feedback loop—with consideration for scalability and reliability', 'Only training and deployment', 'A GPU and a model'], correctAnswer: 1, explanation: 'A complete ML system includes: data collection/validation, feature pipelines (offline + online), training pipeline (experiment tracking, hyperparameter tuning), model serving (API, batch), monitoring (drift, performance), feedback loops, and infrastructure (compute, storage).', difficulty: 'medium', roles: ['ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'system-design', lessonId: 'design-principles' },
  { id: 'sd-3', text: 'How would you design a fraud detection system?', options: ['Use simple rules', 'Real-time scoring with low latency, combining rule-based systems with ML models, using features from transaction history, device info, and behavioral patterns, with feedback loops for continuous improvement', 'Only check transaction amount', 'Batch processing once daily'], correctAnswer: 1, explanation: 'Fraud detection: (1) Real-time feature computation (transaction velocity, device fingerprint, location), (2) Rule engine for known patterns, (3) ML model (gradient boosting or neural network) for scoring, (4) Human review for edge cases, (5) Feedback loop for model updates. Must handle extreme class imbalance.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer'], category: 'case_study', companySizes: ['large', 'faang'], unitId: 'system-design', lessonId: 'design-principles' },
  { id: 'sd-4', text: 'What is the training-serving skew problem?', options: ['Skewed training data', 'Differences between the feature computation during training and serving that lead to degraded production performance', 'Model bias', 'Data imbalance'], correctAnswer: 1, explanation: 'Training-serving skew occurs when features are computed differently in training vs. serving (different code, different data joins, timing issues). This causes models to perform worse in production than in evaluation. Feature stores help ensure consistency.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'system-design', lessonId: 'design-principles' },
  { id: 'sd-5', text: 'How would you design a content moderation system at scale?', options: ['Manual review of everything', 'Multi-layer approach: automated ML classifiers for first pass (text, image, video), confidence-based routing to human reviewers, feedback loop, with hash-matching for known violations', 'Only keyword filtering', 'User reports only'], correctAnswer: 1, explanation: 'Content moderation at scale: (1) Hash matching for known violations (fast), (2) ML classifiers (text toxicity, NSFW images, deepfake detection), (3) Confidence-based routing: high-confidence → auto-action, low-confidence → human review, (4) Appeal process, (5) Continuous model retraining with reviewer feedback.', difficulty: 'hard', roles: ['ml_engineer', 'ai_engineer'], category: 'case_study', companySizes: ['large', 'faang'], unitId: 'system-design', lessonId: 'design-cases' },
  { id: 'sd-6', text: 'How do you choose between real-time and batch prediction?', options: ['Always use real-time', 'Real-time for low-latency needs (fraud, search ranking); batch for high-volume scheduled predictions (email campaigns, reports). Consider latency requirements, volume, cost, and feature freshness.', 'Batch is always cheaper', 'Real-time is always better'], correctAnswer: 1, explanation: 'Real-time: <100ms latency, request-response pattern, good for interactive features. Batch: scheduled, processes millions of predictions, cheaper compute. Some systems use both: batch pre-compute most predictions, real-time for time-sensitive or new items.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'system-design', lessonId: 'design-cases' },

  // More system design
  { id: 'sd-7', text: 'How would you design a search ranking system?', options: ['Just use keyword matching', 'Multi-stage: query understanding → candidate retrieval (inverted index + embedding search) → ranking (learning-to-rank model) → re-ranking (personalization, diversity)', 'Sort by date', 'Use a single neural network'], correctAnswer: 1, explanation: 'Search ranking: (1) Query processing (spelling, expansion, intent), (2) Retrieval: inverted index + semantic search (embeddings, ANN), (3) L1 ranking: lightweight model scores 1000s of docs, (4) L2 ranking: heavy model re-ranks top-N, (5) Final re-ranking: business rules, diversity, freshness.', difficulty: 'very_hard', roles: ['ml_engineer', 'ai_engineer'], category: 'case_study', companySizes: ['faang'], unitId: 'system-design', lessonId: 'design-cases' },
  { id: 'sd-8', text: 'What are the key trade-offs in ML system design?', options: ['There are no trade-offs', 'Accuracy vs latency, model complexity vs interpretability, real-time vs batch, exploration vs exploitation, build vs buy, cost vs performance', 'Only accuracy matters', 'Only cost matters'], correctAnswer: 1, explanation: 'Key trade-offs: accuracy vs. latency (complex models are slower), interpretability vs. performance (black box vs. glass box), online vs. batch (cost vs. freshness), exploration vs. exploitation (in recommendations), custom vs. off-the-shelf, compute cost vs. model quality.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'system-design', lessonId: 'design-cases' },

  // ====== UNIT: ab-testing ======
  { id: 'ab-1', text: 'What is an A/B test?', options: ['Comparing two algorithms', 'A randomized controlled experiment comparing two or more variants to measure the causal impact on a target metric', 'Comparing training and test data', 'A model evaluation method'], correctAnswer: 1, explanation: 'A/B tests randomly assign users to control (A) and treatment (B) groups. By measuring the difference in key metrics between groups, you can attribute the change to the treatment with statistical confidence. The gold standard for causal inference in product decisions.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'ab-testing', lessonId: 'experiment-design' },
  { id: 'ab-2', text: 'How do you calculate the required sample size for an A/B test?', options: ['Just use as many users as possible', 'Based on desired significance level (α), statistical power (1-β), minimum detectable effect (MDE), and baseline metric variance', 'Always 1000 per group', 'Sample size does not matter'], correctAnswer: 1, explanation: 'Sample size depends on: significance level α (usually 0.05), power 1-β (usually 0.80), minimum detectable effect (MDE), and variance. n ≈ (Z_α/2 + Z_β)² × 2σ² / δ². Smaller MDE or higher power requires larger samples. Tools: statsmodels, Evan Miller calculator.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'ab-testing', lessonId: 'experiment-design' },
  { id: 'ab-3', text: 'What is the novelty effect in A/B testing?', options: ['New features always perform better', 'Users may initially engage more with a new feature simply because it is new, leading to inflated short-term metrics that do not reflect long-term behavior', 'A statistical test', 'A type of bias in data'], correctAnswer: 1, explanation: 'The novelty effect causes artificially high engagement when a new feature launches. Users explore it out of curiosity, inflating metrics temporarily. Solutions: run experiments long enough, look for metric stabilization over time, use holdback groups.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'ab-testing', lessonId: 'experiment-design' },
  { id: 'ab-4', text: 'What is Simpson\'s Paradox and how can it affect A/B tests?', options: ['A TV show reference', 'A trend that appears in different groups reverses when the groups are combined, potentially misleading overall A/B test results if there are confounding segments', 'A statistical test', 'A paradox about sample size'], correctAnswer: 1, explanation: 'Simpson\'s Paradox: treatment wins in every subgroup but loses overall (or vice versa) due to different group sizes. In A/B tests, this can happen with uneven segment distribution. Solution: always segment results by key dimensions (device, country, user type).', difficulty: 'very_hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['faang'], unitId: 'ab-testing', lessonId: 'experiment-design' },
  { id: 'ab-5', text: 'What is p-hacking and how do you avoid it?', options: ['Hacking p-values', 'Manipulating analyses until finding a statistically significant result through multiple testing, data dredging, or early stopping; avoided by pre-registration and correction methods', 'A cybersecurity concern', 'A feature engineering technique'], correctAnswer: 1, explanation: 'P-hacking: running many analyses until finding p<0.05 (trying different metrics, segments, stopping rules). Prevention: pre-register hypotheses and analysis plan, use sequential testing for early stopping, apply multiple testing corrections, establish primary metrics upfront.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'ab-testing', lessonId: 'experiment-design' },
  { id: 'ab-6', text: 'What is the difference between practical significance and statistical significance?', options: ['They are the same', 'Statistical significance means the effect is unlikely due to chance; practical significance means the effect is large enough to be meaningful for business decisions', 'Practical significance is not important', 'Statistical significance requires larger samples'], correctAnswer: 1, explanation: 'A result can be statistically significant (p<0.05) but practically insignificant (e.g., 0.01% conversion increase). With large samples, tiny effects become significant. Always consider: is the effect size large enough to justify implementation cost and complexity?', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'ab-testing', lessonId: 'analysis-pitfalls' },
  { id: 'ab-7', text: 'What is network effects interference in A/B tests?', options: ['Internet connectivity issues', 'When treatment on one user affects the outcomes of other users (e.g., in social networks), violating the independence assumption of standard A/B tests', 'Network latency', 'Server load issues'], correctAnswer: 1, explanation: 'Network interference (SUTVA violation): in social/marketplace products, treating user A affects user B\'s experience. Example: showing user A a new messaging feature changes B\'s inbox too. Solutions: cluster randomization (randomize by groups/geography), switchback designs.', difficulty: 'very_hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['faang'], unitId: 'ab-testing', lessonId: 'analysis-pitfalls' },
  { id: 'ab-8', text: 'What are guardrail metrics in A/B testing?', options: ['Safety barriers', 'Metrics that must not degrade significantly during an experiment (e.g., page load time, crash rate), acting as safety checks even if the primary metric improves', 'The same as primary metrics', 'Optional metrics'], correctAnswer: 1, explanation: 'Guardrail metrics are "do no harm" checks: latency, crash rate, revenue, user retention. Even if the primary metric (e.g., engagement) improves, you should not ship if guardrails degrade. They protect against unintended negative consequences.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'ab-testing', lessonId: 'analysis-pitfalls' },
  { id: 'ab-9', text: 'What is an interleaving experiment?', options: ['Running tests simultaneously', 'Mixing results from different rankers/models within the same user session (e.g., alternating search results) for faster, more sensitive comparison than traditional A/B tests', 'Interleaving code changes', 'A type of canary deployment'], correctAnswer: 1, explanation: 'Interleaving is used for ranking systems (search, recommendations). Instead of splitting users, both models contribute results to the same user, interleaved. Users implicitly "vote" by which results they click. Requires 10-100× fewer samples than traditional A/B tests.', difficulty: 'very_hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['faang'], unitId: 'ab-testing', lessonId: 'analysis-pitfalls' },
  { id: 'ab-10', text: 'What is a multi-armed bandit approach and how does it differ from A/B testing?', options: ['A gambling strategy', 'An adaptive algorithm that dynamically allocates more traffic to better-performing variants during the experiment, optimizing for reward while still learning', 'The same as A/B testing', 'A deployment strategy'], correctAnswer: 1, explanation: 'Unlike A/B tests (fixed allocation), bandits adaptively shift traffic toward winning variants, reducing regret. Thompson Sampling and UCB are common algorithms. Trade-off: A/B gives cleaner statistical inference, bandits optimize for total reward during the experiment.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'ab-testing', lessonId: 'analysis-pitfalls' },

  // ====== UNIT: business-product ======
  { id: 'bp-1', text: 'How do you translate a business problem into a machine learning problem?', options: ['Apply any ML algorithm', 'Define the business objective, identify what to predict, determine available data, select appropriate ML formulation (classification/regression/ranking), and define success metrics', 'Start with the most complex model', 'Business and ML problems are the same'], correctAnswer: 1, explanation: 'Steps: (1) Understand business goal and constraints, (2) Define ML task type, (3) Identify features and label, (4) Determine data availability and quality, (5) Define offline metrics (proxy for business KPIs), (6) Establish baselines, (7) Plan for production deployment and monitoring.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'case_study', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'business-product', lessonId: 'product-metrics' },
  { id: 'bp-2', text: 'What is the difference between a vanity metric and an actionable metric?', options: ['They are the same', 'Vanity metrics look impressive but do not inform decisions (total users); actionable metrics drive specific business actions (conversion rate, retention rate)', 'Vanity metrics are always wrong', 'Actionable metrics are harder to compute'], correctAnswer: 1, explanation: 'Vanity metrics: total page views, total signups (grow naturally, hard to act on). Actionable metrics: conversion rate, DAU/MAU ratio, churn rate, revenue per user (directly tied to business decisions). Always focus on metrics that drive decisions.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer'], category: 'case_study', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'business-product', lessonId: 'product-metrics' },
  { id: 'bp-3', text: 'How do you measure the ROI of an ML project?', options: ['Only look at model accuracy', 'Compare the incremental business value (revenue increase, cost reduction, efficiency gain) against the total cost (development, infrastructure, maintenance)', 'ROI cannot be measured for ML', 'Only measure development time'], correctAnswer: 1, explanation: 'ML ROI = (Business Value - Total Cost) / Total Cost. Business value: revenue uplift from better recommendations, cost savings from automation, reduced fraud losses. Costs: engineer time, compute, data labeling, ongoing maintenance. Must establish baselines to measure incremental impact.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'case_study', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'business-product', lessonId: 'product-metrics' },
  { id: 'bp-4', text: 'A company wants to reduce customer churn. How would you approach this with ML?', options: ['Predict who will churn and do nothing', 'Build a churn prediction model, identify key churn drivers, design targeted interventions, measure impact through A/B tests, and create a feedback loop', 'Just offer discounts to everyone', 'Only analyze historical data'], correctAnswer: 1, explanation: 'Approach: (1) Define churn, (2) Build prediction model (features: usage patterns, support tickets, payment history), (3) Identify top churn risk factors for intervention design, (4) Target at-risk customers with personalized retention offers, (5) A/B test interventions, (6) Measure cost-effectiveness.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'case_study', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'business-product', lessonId: 'business-cases' },
  { id: 'bp-5', text: 'How would you prioritize ML projects for a company?', options: ['Build the most technically interesting one', 'Evaluate based on business impact (revenue/cost), feasibility (data availability, technical complexity), and time to value, using a framework like RICE scoring', 'Always start with deep learning', 'Build everything in parallel'], correctAnswer: 1, explanation: 'Prioritize using: Impact (business value), Confidence (data availability, feasibility), Effort (development time, cost). RICE framework: Reach × Impact × Confidence / Effort. Start with high-impact, low-effort projects ("quick wins") to build credibility and learn.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'case_study', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'business-product', lessonId: 'business-cases' },

  // More business questions
  { id: 'bp-6', text: 'What metrics would you use to evaluate a search engine?', options: ['Only click-through rate', 'NDCG, MRR, CTR, time to first click, zero-result rate, session success rate, query abandonment rate, and long-click rate', 'Only accuracy', 'Page views'], correctAnswer: 1, explanation: 'Search metrics: NDCG (ranking quality), MRR (position of first relevant result), CTR (clicks per query), time to first click, zero-result rate, abandonment rate, dwell time/long-click rate (quality of clicked results), session success rate.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer'], category: 'case_study', companySizes: ['large', 'faang'], unitId: 'business-product', lessonId: 'product-metrics' },
  { id: 'bp-7', text: 'How would you detect and handle data quality issues in production?', options: ['Assume data is always clean', 'Implement automated data validation checks (schema, distribution, completeness), set up alerts for anomalies, and have fallback strategies', 'Only check during development', 'Manual inspection daily'], correctAnswer: 1, explanation: 'Data quality in production: (1) Schema validation (Great Expectations, Deequ), (2) Statistical checks (distribution shifts, null rates, cardinality changes), (3) Freshness monitoring, (4) Automated alerts, (5) Fallback strategies (use cached data, degrade gracefully). Garbage in = garbage out.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'business-product', lessonId: 'business-cases' },
  { id: 'bp-8', text: 'You notice your model\'s production performance dropped last week. How do you debug it?', options: ['Retrain immediately', 'Investigate systematically: check for data issues (drift, quality), infrastructure problems, upstream changes, compare feature distributions, and analyze error patterns by segment', 'Ignore it if accuracy is still above 90%', 'Replace the model with a simpler one'], correctAnswer: 1, explanation: 'Debugging flow: (1) Confirm the drop is real (not a metric bug), (2) Check for data issues (missing data, schema changes, upstream pipeline failures), (3) Check for data drift, (4) Analyze by segment (is it uniform or specific to a subgroup?), (5) Check infrastructure (latency, timeouts), (6) If data-related, retrain; if code-related, fix and redeploy.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'case_study', companySizes: ['large', 'faang'], unitId: 'business-product', lessonId: 'business-cases' },

  // ====== UNIT: behavioral ======
  { id: 'bh-1', text: 'Tell me about a time you had to explain a complex technical concept to a non-technical stakeholder. What approach works best?', options: ['Use as much jargon as possible to seem credible', 'Use analogies, visual aids, focus on business impact rather than technical details, and check for understanding iteratively', 'Avoid explaining and just give results', 'Send them documentation to read'], correctAnswer: 1, explanation: 'Best practice: start with the "why" (business impact), use relatable analogies, visual aids (before/after, simple charts), avoid jargon, be iterative (check understanding, adjust explanation), focus on what they can do with the information.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'behavioral-core' },
  { id: 'bh-2', text: 'What is the best approach when a project deadline is at risk?', options: ['Work overtime without telling anyone', 'Communicate early, reassess scope and priorities, propose alternatives (reduce scope, extend deadline, add resources), and be transparent about trade-offs', 'Just deliver whatever is ready', 'Blame the team for slow progress'], correctAnswer: 1, explanation: 'Key: early communication with stakeholders. Assess what can be descoped, propose options (MVP first, phased delivery), identify blockers that can be removed, and be transparent about trade-offs. Surprising stakeholders at the deadline is always worse than early communication.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'behavioral-core' },
  { id: 'bh-3', text: 'How do you handle disagreements with team members about technical approaches?', options: ['Insist your way is right', 'Listen actively, understand their perspective, use data and evidence to support your view, propose experiments/POCs to settle debates, and align on evaluation criteria', 'Escalate immediately to manager', 'Give in to avoid conflict'], correctAnswer: 1, explanation: 'Approach: (1) Listen and understand their reasoning, (2) Articulate your perspective with data/evidence, (3) Identify shared goals, (4) If unresolved, propose a small experiment or POC, (5) Agree on evaluation criteria upfront. Disagree and commit once a decision is made.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'behavioral-core' },
  { id: 'bh-4', text: 'Describe a situation where your ML model did not perform as expected in production. What did you do?', options: ['Blamed the data team', 'Investigated systematically (data quality, drift, feature issues), communicated the issue transparently, implemented a fix, and set up monitoring to prevent recurrence', 'Ignored it and moved to the next project', 'Retrained until it worked'], correctAnswer: 1, explanation: 'Structure: (1) Impact assessment—how bad is it? (2) Root cause analysis, (3) Short-term fix (rollback, hotfix), (4) Long-term fix (retrain, pipeline fix), (5) Post-mortem to learn, (6) Monitoring/alerts to catch earlier next time. Transparency and learning are key.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'behavioral-core' },
  { id: 'bh-5', text: 'How do you stay current with rapidly evolving ML/AI technology?', options: ['I don\'t need to stay current', 'Read papers (arXiv), follow key researchers on social media, attend conferences/meetups, take online courses, do hands-on projects, and participate in community discussions', 'Only attend annual conferences', 'Wait for my company to train me'], correctAnswer: 1, explanation: 'Good answers include a mix of: reading arXiv papers and ML blogs, following researchers on Twitter/X, listening to ML podcasts, taking courses (fast.ai, Coursera), hands-on experimentation, contributing to open source, attending meetups/conferences, and internal knowledge sharing.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'behavioral-core' },

  // More behavioral
  { id: 'bh-6', text: 'You are given a tight deadline and multiple competing priorities. How do you manage your time?', options: ['Work on everything simultaneously', 'Assess impact and urgency of each task, communicate priorities to stakeholders, break work into milestones, delegate where possible, and focus on high-impact items first', 'Work on the easiest task first', 'Ask the manager to decide everything'], correctAnswer: 1, explanation: 'Use Eisenhower matrix (urgent/important), communicate trade-offs to stakeholders, negotiate deadlines if needed, break large tasks into milestones, time-box exploration, and focus on high-impact deliverables. Over-communication is better than surprises.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'behavioral-core' },
  { id: 'bh-7', text: 'How would you lead a cross-functional team on an ML project?', options: ['Make all decisions yourself', 'Establish clear goals and communication channels, understand each team\'s constraints, create shared documentation, run regular syncs, and build trust through transparency', 'Let each team work independently', 'Only communicate through email'], correctAnswer: 1, explanation: 'Cross-functional leadership: (1) Align on shared goals and metrics, (2) Understand each team\'s needs/constraints, (3) Regular standups/syncs with clear agendas, (4) Shared documentation (design docs, decision logs), (5) Build personal relationships, (6) Celebrate shared wins.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'leadership-teamwork' },
  { id: 'bh-8', text: 'How do you handle a situation where a stakeholder wants you to use ML when a simpler solution would suffice?', options: ['Always use ML to show your skills', 'Explain the trade-offs, demonstrate the simpler solution\'s effectiveness, discuss maintenance costs of ML, and recommend ML only when it provides clear incremental value', 'Refuse the project', 'Build both and let them choose'], correctAnswer: 1, explanation: 'Best approach: (1) Start with simpler baseline (rules, heuristics, SQL), (2) Show it solves 80% of the problem, (3) Explain ML overhead (training, serving, monitoring, maintenance), (4) Quantify incremental value of ML, (5) Recommend ML only if ROI justifies complexity.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'leadership-teamwork' },
  { id: 'bh-9', text: 'Describe a time when you had to mentor or teach a less experienced team member.', options: ['I don\'t have time for mentoring', 'Share context not just answers, create safe spaces for questions, pair program, give constructive feedback, set clear expectations, and celebrate their growth', 'Just point them to documentation', 'Do their work for them'], correctAnswer: 1, explanation: 'Effective mentoring: provide context (the "why"), use pair programming, create safe space for questions, give specific constructive feedback, set achievable goals, check in regularly, gradually increase challenge level, and celebrate progress. Teaching also deepens your own understanding.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'leadership-teamwork' },
  { id: 'bh-10', text: 'What do you do when you receive ambiguous requirements for a project?', options: ['Make assumptions and start building', 'Ask clarifying questions, document assumptions, propose a clear problem statement, get alignment from stakeholders, and plan for iterative refinement', 'Wait until requirements are clearer', 'Escalate immediately'], correctAnswer: 1, explanation: 'Handle ambiguity: (1) List what you know and don\'t know, (2) Ask specific clarifying questions, (3) Document assumptions explicitly, (4) Propose a problem statement and success criteria, (5) Get stakeholder buy-in, (6) Plan for iteration (start with MVP, refine based on feedback).', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'behavioral', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'behavioral', lessonId: 'leadership-teamwork' },

  // ====== UNIT: cloud-infrastructure ======
  { id: 'cl-1', text: 'What is AWS SageMaker?', options: ['A general computing service', 'A fully managed ML platform providing tools for data labeling, training, hyperparameter tuning, deployment, and monitoring of ML models', 'A database service', 'A networking tool'], correctAnswer: 1, explanation: 'SageMaker provides: notebook instances, built-in algorithms, training jobs (distributed), hyperparameter tuning, model hosting (endpoints), batch transform, model monitoring, feature store, pipelines, and Ground Truth (labeling). The primary AWS ML platform.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'cloud-ml-services' },
  { id: 'cl-2', text: 'What are the key differences between GPU and CPU for ML workloads?', options: ['GPUs are always better', 'GPUs have thousands of cores for parallel computation (ideal for matrix operations in training); CPUs have fewer, more powerful cores (better for sequential logic and inference)', 'CPUs are only for inference', 'They perform the same'], correctAnswer: 1, explanation: 'GPUs: thousands of simple cores for parallel matrix operations, essential for training neural networks. CPUs: fewer but more powerful cores, good for data preprocessing, small models, inference. TPUs (Google): specialized for tensor operations. Choose based on workload.', difficulty: 'easy', roles: ['ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'cloud-ml-services' },
  { id: 'cl-3', text: 'What is a vector database and why is it important for AI applications?', options: ['A traditional relational database', 'A database optimized for storing and querying high-dimensional vectors (embeddings), enabling fast similarity search for RAG, recommendations, and semantic search', 'A CSV file store', 'Only for computer vision'], correctAnswer: 1, explanation: 'Vector databases (Pinecone, Weaviate, Milvus, Chroma) store embeddings and enable fast approximate nearest neighbor (ANN) search. Critical for: RAG (finding relevant documents), semantic search, recommendation systems, image similarity. Use indexing methods like HNSW, IVF.', difficulty: 'medium', roles: ['ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'cloud-ml-services' },
  { id: 'cl-4', text: 'What is serverless computing and when is it suitable for ML?', options: ['Computing without servers', 'Cloud functions that automatically scale and charge per-execution (AWS Lambda, Cloud Functions), suitable for lightweight ML inference, preprocessing, and event-driven workflows', 'Always better than containers', 'Not suitable for ML'], correctAnswer: 1, explanation: 'Serverless (Lambda, Cloud Functions): auto-scales, pay per execution, no server management. Good for: lightweight inference (<15min), preprocessing triggers, low-traffic endpoints. Limitations: cold starts, memory limits, execution time limits. Not ideal for GPU workloads or high-throughput.', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'cloud-ml-services' },
  { id: 'cl-5', text: 'What is infrastructure as code (IaC) and why is it important for ML?', options: ['Writing code on infrastructure', 'Defining infrastructure (servers, databases, networks) in version-controlled code (Terraform, CloudFormation) for reproducibility, consistency, and automated provisioning', 'A programming paradigm', 'Only for web applications'], correctAnswer: 1, explanation: 'IaC (Terraform, Pulumi, CloudFormation) manages infrastructure through code. For ML: reproducible training environments, consistent staging/production, automated GPU provisioning, version-controlled infrastructure changes, easy environment replication. Essential for mature MLOps.', difficulty: 'hard', roles: ['mlops_engineer', 'ml_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'infrastructure-scaling' },

  // More cloud/infra
  { id: 'cl-6', text: 'How do you choose between spot/preemptible instances and on-demand instances for ML training?', options: ['Always use on-demand', 'Spot instances are 60-90% cheaper but can be interrupted; use for fault-tolerant workloads (training with checkpointing). On-demand for critical, time-sensitive workloads.', 'Spot instances are never reliable', 'Cost does not matter for ML'], correctAnswer: 1, explanation: 'Spot/preemptible instances: 60-90% cheaper, can be interrupted with 2-minute notice. Strategy: use for training with regular checkpointing (save every N steps), data processing, hyperparameter search. Use on-demand for: serving, time-critical training, short jobs not worth checkpointing.', difficulty: 'hard', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'infrastructure-scaling' },
  { id: 'cl-7', text: 'What is auto-scaling for ML model serving?', options: ['Manually adding servers', 'Automatically adjusting the number of serving instances based on traffic load to maintain performance while optimizing cost', 'Scaling model size', 'Increasing GPU memory'], correctAnswer: 1, explanation: 'Auto-scaling adjusts serving replicas based on metrics (CPU utilization, request queue length, custom metrics). Horizontal scaling: add more instances. Key settings: min/max replicas, scaling thresholds, cooldown periods. Prevents both over-provisioning (waste) and under-provisioning (degraded service).', difficulty: 'medium', roles: ['ml_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'cloud-infrastructure', lessonId: 'infrastructure-scaling' },

  // ====== UNIT: recommendation-systems ======
  { id: 'rs-1', text: 'What is collaborative filtering?', options: ['Filtering spam collaboratively', 'Recommending items based on the preferences of similar users (user-based) or similar items (item-based), using the assumption that similar users like similar items', 'A content analysis method', 'Filtering data in parallel'], correctAnswer: 1, explanation: 'Collaborative filtering uses user-item interaction data. User-based: find similar users, recommend what they liked. Item-based: find similar items to what user liked. Does not need item content features. Challenges: cold start, sparsity. Matrix factorization is a popular implementation.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-fundamentals' },
  { id: 'rs-2', text: 'What is the cold start problem in recommendation systems?', options: ['Server startup time', 'The difficulty of making recommendations for new users (no history) or new items (no interactions), since collaborative filtering requires interaction data', 'Low server temperature', 'Slow model inference'], correctAnswer: 1, explanation: 'Cold start: new users (no preference data) and new items (no interaction data). Solutions: content-based features (demographics, item attributes), popular items as default, onboarding questionnaire, hybrid systems, explore-exploit strategies (bandits).', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-fundamentals' },
  { id: 'rs-3', text: 'What is matrix factorization in recommendations?', options: ['Splitting a matrix into rows', 'Decomposing the user-item interaction matrix into lower-dimensional user and item embeddings, where the dot product approximates the original interactions', 'A preprocessing step', 'An evaluation metric'], correctAnswer: 1, explanation: 'Matrix factorization decomposes user-item matrix R ≈ U × V^T, where U (user embeddings) and V (item embeddings) are low-rank. Missing entries are predicted by dot product of embeddings. SVD-based approaches, ALS, and neural matrix factorization are common implementations.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-fundamentals' },
  { id: 'rs-4', text: 'What is content-based filtering?', options: ['Filtering content by language', 'Recommending items similar to what the user has previously liked, based on item features/attributes (genre, description, category) rather than other users\' behavior', 'A collaborative filtering variant', 'Filtering user-generated content'], correctAnswer: 1, explanation: 'Content-based filtering uses item features to find similar items. If you liked action movies, recommend other action movies. Advantages: no cold start for items (features available), transparent. Disadvantages: limited diversity, cannot discover unexpected interests, requires good item features.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-fundamentals' },
  { id: 'rs-5', text: 'What is the exploration-exploitation tradeoff in recommendations?', options: ['Exploring new databases', 'Balancing between showing items the system is confident the user will like (exploitation) vs. showing new/diverse items to learn user preferences (exploration)', 'Only showing popular items', 'A computational trade-off'], correctAnswer: 1, explanation: 'Exploitation: recommend what the model predicts the user will like (safe but creates filter bubbles). Exploration: show diverse/novel items to discover new preferences (risky but informative). Techniques: epsilon-greedy, Thompson sampling, UCB. Key for long-term engagement.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-advanced' },

  // More recsys
  { id: 'rs-6', text: 'What is a two-tower model in recommendations?', options: ['A model with two layers', 'An architecture with separate user and item encoder towers that produce embeddings, enabling efficient candidate retrieval via approximate nearest neighbor search', 'A model running on two servers', 'A type of GAN'], correctAnswer: 1, explanation: 'Two-tower models encode users and items into separate embedding spaces. At serving time, item embeddings are pre-computed and indexed. For a user, compute their embedding and find nearest items via ANN search. This enables retrieval from millions of candidates in milliseconds.', difficulty: 'hard', roles: ['ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-advanced' },
  { id: 'rs-7', text: 'What metrics are used to evaluate recommendation systems?', options: ['Only accuracy', 'Precision@K, Recall@K, NDCG, MAP, Hit Rate, coverage, diversity, novelty, and business metrics like CTR and conversion rate', 'Only click-through rate', 'F1 score only'], correctAnswer: 1, explanation: 'Offline: Precision@K, Recall@K, NDCG (ranking quality), MAP, Hit Rate, MRR. Beyond accuracy: coverage (% items recommended), diversity (variety), novelty (newness), serendipity (surprise). Online: CTR, conversion rate, engagement time, revenue.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'recommendation-systems', lessonId: 'recsys-advanced' },

  // ====== UNIT: time-series ======
  { id: 'ts-1', text: 'What is stationarity in time series and why does it matter?', options: ['Data that does not change', 'A stationary series has constant mean, variance, and autocorrelation over time; many models (ARIMA) require stationarity for valid results', 'Data that increases linearly', 'Data with no missing values'], correctAnswer: 1, explanation: 'Stationarity means statistical properties (mean, variance, autocorrelation) do not change over time. Most classical time series models assume stationarity. Non-stationary data can be made stationary through differencing, log transforms, or detrending.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'time-series', lessonId: 'ts-fundamentals' },
  { id: 'ts-2', text: 'What is ARIMA and what do its components mean?', options: ['A visualization library', 'AutoRegressive Integrated Moving Average: AR (past values), I (differencing for stationarity), MA (past errors). ARIMA(p,d,q) where p=AR order, d=differencing, q=MA order', 'A neural network', 'A clustering algorithm'], correctAnswer: 1, explanation: 'ARIMA(p,d,q): AR(p) models dependency on p past values. I(d) applies d differences for stationarity. MA(q) models dependency on q past forecast errors. SARIMA adds seasonal components. Use ACF/PACF plots or auto_arima to select p,d,q.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'time-series', lessonId: 'ts-fundamentals' },
  { id: 'ts-3', text: 'What are lag features in time series?', options: ['Features that are slow to compute', 'Features created from previous time steps (y_{t-1}, y_{t-2}, etc.) that capture temporal dependencies for ML models', 'Delayed model predictions', 'Features from the future'], correctAnswer: 1, explanation: 'Lag features use past values as predictors: y_{t-1} (1-step lag), y_{t-7} (weekly lag), etc. Also: rolling statistics (rolling mean, std), expanding features, and time-since features. Critical for making time series data suitable for standard ML models.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'time-series', lessonId: 'ts-fundamentals' },
  { id: 'ts-4', text: 'What is the difference between additive and multiplicative decomposition?', options: ['They are identical', 'Additive: Y = Trend + Seasonal + Residual (constant seasonal amplitude). Multiplicative: Y = Trend × Seasonal × Residual (seasonal amplitude proportional to trend level).', 'Additive is always better', 'Multiplicative is only for negative data'], correctAnswer: 1, explanation: 'Additive decomposition assumes components add together (constant seasonal variations). Multiplicative assumes they multiply (seasonal variations proportional to level). Use additive when seasonal fluctuations are constant, multiplicative when they grow with the level.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'time-series', lessonId: 'ts-fundamentals' },
  { id: 'ts-5', text: 'What is Prophet and when would you use it?', options: ['A prediction API', 'Facebook\'s time series forecasting model that handles trends, seasonality (daily/weekly/yearly), and holidays automatically, good for business time series with strong seasonal patterns', 'A deep learning model', 'A data collection tool'], correctAnswer: 1, explanation: 'Prophet (Meta) decomposes time series into trend (piecewise linear/logistic), seasonality (Fourier series), and holidays. Good for: business data with clear patterns, handling missing data and outliers, quick baselines. Less suitable for: non-seasonal data, very high frequency.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large'], unitId: 'time-series', lessonId: 'ts-advanced' },
  { id: 'ts-6', text: 'How do you prevent data leakage in time series cross-validation?', options: ['Use standard K-fold', 'Never use future data for training; use expanding or sliding window validation where training data always precedes validation data temporally', 'Shuffle the data first', 'Use stratified K-fold'], correctAnswer: 1, explanation: 'Time series leakage: using future information to predict the past. Prevention: always maintain temporal order, use walk-forward validation (expanding window), no shuffling, create features only from past data, be careful with rolling statistics that might look ahead.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer'], category: 'technical', companySizes: ['midsize', 'large', 'faang'], unitId: 'time-series', lessonId: 'ts-advanced' },

  // ====== UNIT: ethics-responsible-ai ======
  { id: 'eth-1', text: 'What is algorithmic bias in ML?', options: ['A bug in the algorithm', 'Systematic errors in ML systems that create unfair outcomes for certain groups, often arising from biased training data, feature selection, or model design choices', 'Preference for one algorithm', 'Bias in the bias-variance tradeoff'], correctAnswer: 1, explanation: 'Algorithmic bias can arise from: biased training data (historical discrimination), biased labels (subjective labeling), underrepresentation of minority groups, proxy features (zip code as proxy for race), and feedback loops that amplify existing biases.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'ethics-responsible-ai', lessonId: 'bias-fairness' },
  { id: 'eth-2', text: 'What is model interpretability and why is it important?', options: ['Making code readable', 'The ability to understand and explain how a model makes predictions; important for trust, debugging, regulatory compliance, and ensuring fairness', 'Only required by law', 'Not important for accurate models'], correctAnswer: 1, explanation: 'Interpretability matters for: building stakeholder trust, debugging model failures, regulatory compliance (GDPR right to explanation), detecting bias, domain expert validation. Methods: SHAP values, LIME, feature importance, attention visualization, decision trees.', difficulty: 'easy', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'ethics-responsible-ai', lessonId: 'bias-fairness' },
  { id: 'eth-3', text: 'What are SHAP values?', options: ['A model accuracy metric', 'SHapley Additive exPlanations: a game-theoretic approach that assigns each feature an importance value for each prediction, providing consistent and locally accurate explanations', 'A regularization technique', 'A hyperparameter'], correctAnswer: 1, explanation: 'SHAP values are based on Shapley values from game theory. For each prediction, each feature gets a SHAP value indicating its contribution. Properties: local accuracy (sum of SHAP values = prediction), consistency, and fairness. Works with any model. More theoretically grounded than LIME.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'ethics-responsible-ai', lessonId: 'bias-fairness' },
  { id: 'eth-4', text: 'What is differential privacy?', options: ['Different models for different users', 'A mathematical framework ensuring that the output of an analysis is roughly the same whether or not any individual\'s data is included, by adding calibrated noise', 'Data encryption', 'Access control for databases'], correctAnswer: 1, explanation: 'Differential privacy adds mathematical noise to computations so that individual records cannot be reverse-engineered from the output. Parameters: ε (privacy budget—lower = more private). Used by Apple, Google, and US Census. Trade-off: more privacy = less utility/accuracy.', difficulty: 'hard', roles: ['data_scientist', 'ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'ethics-responsible-ai', lessonId: 'governance-privacy' },
  { id: 'eth-5', text: 'What is federated learning?', options: ['A distributed database', 'A training approach where models are trained across decentralized devices holding local data, without exchanging raw data, preserving privacy', 'Learning from federal regulations', 'A type of transfer learning'], correctAnswer: 1, explanation: 'Federated learning: (1) Send model to devices, (2) Train locally on device data, (3) Send only model updates (gradients) to server, (4) Aggregate updates, (5) Repeat. Privacy-preserving: raw data never leaves the device. Used by Google (keyboard prediction), Apple (Siri).', difficulty: 'hard', roles: ['ml_engineer', 'ai_engineer'], category: 'technical', companySizes: ['large', 'faang'], unitId: 'ethics-responsible-ai', lessonId: 'governance-privacy' },
  { id: 'eth-6', text: 'What are the key considerations for responsible AI deployment?', options: ['Only accuracy matters', 'Fairness (equitable outcomes), transparency (explainability), privacy (data protection), safety (robustness), accountability (human oversight), and societal impact', 'Just follow the law', 'Let the model decide'], correctAnswer: 1, explanation: 'Responsible AI pillars: (1) Fairness—test for bias across groups, (2) Transparency—explainable decisions, (3) Privacy—data minimization, consent, (4) Safety—adversarial robustness, fail-safes, (5) Accountability—human oversight, audit trails, (6) Societal impact—consider broader consequences.', difficulty: 'medium', roles: ['data_scientist', 'ml_engineer', 'ai_engineer', 'mlops_engineer'], category: 'technical', companySizes: ['startup', 'midsize', 'large', 'faang'], unitId: 'ethics-responsible-ai', lessonId: 'governance-privacy' },
];
